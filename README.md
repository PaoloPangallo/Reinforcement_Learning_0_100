# Reinforcement Learning Lab   
Un laboratorio completo di Reinforcement Learning con implementazioni moderne, visualizzazioni interattive e analisi degli algoritmi piÃ¹ usati nella letteratura.

##  Obiettivo del progetto
Questo repository raccoglie **tutti gli esperimenti RL**, le implementazioni, i viewer interattivi e le analisi comparative sviluppate durante il mio percorso di studio e sperimentazione.  
Lâ€™obiettivo Ã¨ creare un ambiente **didattico, riproducibile e visivo**, che permetta di capire davvero come gli agenti imparano nel tempo.

---

#  Contenuti Principali
- Implementazioni *from scratch* di:
  - **Q-Learning**
  - **DQN**
  - **Double DQN**
  - **Dueling DQN**
  - **Prioritized Experience Replay (PER)**
  - **PPO**
  - **TD3**
  - **SAC**
- Visualizzatori interattivi per:
  - **BipedalWalker TD3 Viewer**
  - **LunarLander Viewer**
  - **Snake PPO Viewer**
- Raccolta automatica di:
  - Video MP4 dei checkpoint
  - JSON delle traiettorie
  - CSV delle metriche
  - Grafici andamento reward
- Studio teorico-pratico di:
  - On-policy vs Off-policy
  - Soft Actor-Critic temperature annealing
  - TD3 policy smoothing
  - RL stability, reward shaping e tuning
  - Confronto fra algoritmi moderni (TD3, SAC, PPO)
  - Limiti e instabilitÃ  note
- Preparazione allâ€™integrazione futura con **DreamerV3** e modelli world-model-based.

---

# ğŸ§© Struttura del Progetto

experiments/
â”‚
â”œâ”€â”€ bipedal_td3/
â”‚ â”œâ”€â”€ metrics/ # Traiettorie JSON per ogni episodio
â”‚ â”œâ”€â”€ models/ # Checkpoint .pth
â”‚ â”œâ”€â”€ videos/ # Video degli episodi registrati
â”‚ â””â”€â”€ plots/ # Grafici reward
â”‚
â”œâ”€â”€ lunarlander_dqn/
â”‚ â”œâ”€â”€ metrics/
â”‚ â”œâ”€â”€ models/
â”‚ â””â”€â”€ videos/
â”‚
â”œâ”€â”€ snake_ppo/
â”‚ â”œâ”€â”€ logs/
â”‚ â”œâ”€â”€ models/
â”‚ â””â”€â”€ viewer/
â”‚
viewer/
â”‚ â”œâ”€â”€ walker/ # Viewer Bipedal TD3
â”‚ â”œâ”€â”€ lunarlander/ # Viewer LunarLander
â”‚ â””â”€â”€ snake/ # Viewer Snake PPO
â”‚
notebooks/
â”‚ â”œâ”€â”€ bipedal_td3_training.ipynb
â”‚ â”œâ”€â”€ sac_vs_td3.ipynb
â”‚ â”œâ”€â”€ ppo_snake.ipynb
â”‚ â””â”€â”€ qlearning_foundations.ipynb


---

# ğŸ§  Algoritmi Implementati

## ğŸ”· Q-Learning & DQN Family
Implementazioni *from scratch* con:
- Replay Buffer
- Target network
- Îµ-greedy scheduling
- Soft updates (Ï„)
- Dueling architecture
- PER (Prioritized Replay Î±/Î² annealing)

## ğŸ”¶ PPO
- Clipped surrogate objective  
- Advantage GAE(Î»)  
- Entropy bonus  
- Multi-batch rollout con normalizzazione  

Usato per il progetto **Snake PPO** (viewer incluso).

## ğŸ”· TD3 (Twin Delayed Deep Deterministic Policy Gradient)
Implementazione con:
- Policy delay
- Target policy smoothing
- Clipped double Q-learning
- Gaussian exploration decay
- Training stabile su BipedalWalker-v3

Include viewer interattivo con:
- Traiettorie passo-passo
- Posizione del walker
- Costo azioni
- Reward cumulativo

## ğŸ”¶ SAC (Soft Actor-Critic)
Implementazione con:
- Entropy temperature learnable (Î±)
- Soft Q update
- Stochastic policy Gaussian
- Reparametrization trick
- Replay buffer condiviso

---

# ğŸ¥ Viewer RL Interattivi (Flask)

## **BipedalWalker â€“ TD3 Viewer**
- Selezione run + episodio  
- Riproduzione frame-by-frame  
- Scrolling del mondo  
- Reward cumulativo  
- Marker del traguardo  

## **LunarLander â€“ DQN Viewer**
- Animazione atterraggio  
- Lettura dei checkpoint  
- Reward ad ogni step  

## **Snake PPO Viewer**
- Rendering griglia  
- Stato PPO (policy, value)  
- Step e punteggi  

---

#  Logging, Analisi e Metriche

Ogni esperimento salva automaticamente:
- `trajectory_ep_XXX.json`  
- `episode_reward.csv`  
- `losses.csv`  
- Video `.mp4`  
- Grafici della curva dei reward  

Con un runner centralizzato per registrare i checkpoint:

```bash
python record_learning_progress.py --algo td3 --env BipedalWalker-v3 ğŸ” Analisi Comparativa degli Algoritmi
On-Policy (PPO)

Stabile

Facilissimo da far convergere
âˆ’ Richiede molti campioni
âˆ’ Adatto ad ambienti stabili

Off-Policy (TD3, SAC)

Molto sample-efficient

Performance migliori

PiÃ¹ flessibile
âˆ’ Tuning piÃ¹ delicato
âˆ’ Richiede piÃ¹ memoria/computazione

Per BipedalWalker

TD3 â†’ eccellente e semplice da stabilizzare

SAC â†’ molto potente ma piÃ¹ sensibile

DreamerV3 (futura integrazione)

Potenziale superiore nei continui

Training piÃ¹ complesso e pesante (JAX/XLA)

ğŸš€ Come Avviare i Viewer
âš™ï¸ Installazione
pip install -r requirements.txt

â–¶ï¸ Avvio Flask Viewer
flask --app viewer/walker/walker_app.py run

â–¶ï¸ Avvio Snake PPO Viewer
flask --app viewer/snake/snake_app.py run



ğŸ“„ Licenza

MIT License.

â­ Credits

Progetto sviluppato come laboratorio personale di Reinforcement Learning, con lâ€™obiettivo di unire teoria, codice e visualizzazioni interattive.
